# chatgpt-paper
本项目主要总结ChatGPT相关论文和学习路线
## survey
* [A Survey of Large Language Models](https://arxiv.org/pdf/2303.18223.pdf)
## 基础模型
* [GPT-1](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)
* [GPT-2](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
* [GPT-3](https://arxiv.org/pdf/2005.14165.pdf)
## instruction learning
* [instructGPT](https://arxiv.org/pdf/2203.02155.pdf)
* [self-instruct](https://arxiv.org/pdf/2212.10560.pdf)
## 轻量级训练大语言模型
* [LoRA](https://arxiv.org/abs/2106.09685)
## 并行训练技术
* [ZeRO: Memory Optimizations Toward Training Trillion Parameter Models](https://arxiv.org/pdf/1910.02054.pdf)
* [GPipe: Easy Scaling with Micro-Batch Pipeline Parallelism](https://arxiv.org/pdf/1811.06965.pdf)
* [Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelis](https://arxiv.org/pdf/1909.08053.pdf)
## 其他
* [Claude](https://arxiv.org/pdf/2212.08073.pdf)
