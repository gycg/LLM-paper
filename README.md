# chatgpt-paper
本项目主要总结ChatGPT相关论文和学习路线
## 论文
### survey
* [A Survey of Large Language Models](https://arxiv.org/pdf/2303.18223.pdf)[[notes](notes/A_Survey_of_Large_Language_Models.md)]

### 基础模型
* [Transformers](https://arxiv.org/pdf/1706.03762.pdf)[[blog:illustrated-transformer](https://jalammar.github.io/illustrated-transformer/)]
* [GPT-1](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)[[notes](notes/GPT-1.md)]
* [GPT-2](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
* [GPT-3](https://arxiv.org/pdf/2005.14165.pdf)

### instruction learning
* [instructGPT](https://arxiv.org/pdf/2203.02155.pdf)
* [self-instruct](https://arxiv.org/pdf/2212.10560.pdf)

### 轻量级训练技术
* [LoRA](https://arxiv.org/abs/2106.09685)

### 并行训练技术
* [ZeRO: Memory Optimizations Toward Training Trillion Parameter Models](https://arxiv.org/pdf/1910.02054.pdf)
* [GPipe: Easy Scaling with Micro-Batch Pipeline Parallelism](https://arxiv.org/pdf/1811.06965.pdf)
* [Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelis](https://arxiv.org/pdf/1909.08053.pdf)

### 量化技术
* [SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models](https://arxiv.org/pdf/2211.10438.pdf)

### 其他LLM
* [Claude](https://arxiv.org/pdf/2212.08073.pdf)
* [LLaMA](https://arxiv.org/pdf/2302.13971.pdf)
* [LLaMA 2](https://arxiv.org/pdf/2307.09288.pdf)
* [Baichuan2](https://arxiv.org/pdf/2309.10305.pdf)

## 代码

### 部署
* [llama.cpp](https://github.com/ggerganov/llama.cpp)
* [gpt4all](https://github.com/nomic-ai/gpt4all)
